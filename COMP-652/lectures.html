<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.o1;2crg/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="index.css" rel="stylesheet" type="text/css" />
<title>Machine Learning (COMP-652 and ECSE-608)</title>

</head>

  
  
  <body>
    <div id="masthead">

      Machine Learning (COMP-652 and ECSE-608)<br>
Fall 2017
      </div>

    <hr/>

    <div id="menu">
      <!--enter your hyperlinks here-->
      <ul>
	<li><a href="index.html"><p>Home</p></a></li>
       <li><a href="syllabus.html"><p>Syllabus</p></a></li>
	<li><a href="lectures.html"><p>Lectures</p></a></li>
        <li><a href="assignments.html"><p>Assignments</p></a></li>
	<li><a href="project.html"><p>Project</p></a></li>
	<li><a href="resources.html"><p>Resources</p></a></li>
	</ul>
      <!--end of hyperlinks-->
      </div>

<div id="mainText">

<center><h2>Lecture Schedule</h2></center>

      <table border='0' cellspacing='0' cellpadding='0'>
	<tr>
	  <td width=8%><b>Date</b></td>
	  <td width=32%><b>Topic</b></td>
	  <td width=52%><b>Materials</b></td>
	  </tr>
<tr><td>Sep. 6</td><td>Introduction. Linear Models.</td>
<td>
<a href="lectures/lecture-1.pdf">Lecture 1 slides</a> -
Bishop, Sec. 1.1, 3.1, 3.2 (or equivalent) <br><br>
If you need to catch up on the math:
<ul>
<li> A brief <a href="http://cs229.stanford.edu/section/cs229-prob.pdf "> probability review</a> from Stanford University;
<li> <a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">Linear algebra and matrix calculus review</a> also from Stanford;</li>
<li> Bishop appendix B,C.</li>
</ul>
</td></tr>

<tr><td>Sep. 11</td><td>More on Linear Models. Overfitting. Regularization.</td>
<td>

<a href="lectures/lecture-2.pdf">Lecture 2 slides</a> -
Bishop, Sec. 1.3, 3.1, 3.2, Hastie Sec. 3.4, 7.1-3, 7.10<br><br>
If you need to catch up on the math:
<ul>
<li> <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint"> Lagrange multipliers</a> on Khan academy;</li>
<li> Bishop appendix E</li>


</ul>
</td></tr>

<tr><td>Sep. 13</td><td>Bayesian and Probabilistic ML. </td>
<td>
<a href="lectures/lecture-3.pdf">Lecture 3 slides</a> - 
Related lectures and materials <br>
<ul>
<li> Book:Bishop PRML : Section 1.2 (Probability Theory); </li>
<li> Book:Barber BRML : Chapter 1 (Probabilistic Reasoning); </li>
<li> Video  <a href="https://www.youtube.com/watch?v=mgBrXnjF8R4 "> Bayesian Inference</a> from Zoubin Ghahramani;</li>
<li> Book: Bishop PRML: Section 2.3 (The Gaussian Distribution). This is a truly excellent and in-depth discussion! </li>
<li> Book: Bishop PRML: Section 3.3 (Bayesian Linear Regression).</li>
<li> Nando de Freitas has a series of lectures on Bayesian linear regression.</li>
</ul>	
</td></tr>
	      
	      
	      
<tr><td>Sep. 18</td><td>Logistic Regression.</td>
<td>

<a href="lectures/lecture-4.pdf">Lecture 4 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Sec. 4.1.1-4.1.3: Linear models for classification</li>
<li> Bishop, Sec. 4.3.2-4.3.4: Logistic regression </li>
<li> Bishop, Sec. 1.6: Information theory (optional for now but will be relevant for future lectures...)</li>
</ul>

</td></tr>


<tr><td>Sep. 20</td><td>Kernels and Support Vector Machines</td>
<td>
<a href="lectures/lecture-5.pdf">Lecture 5 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Sec. 6.1-6.2: Kernels </li>
<li> Bishop, Sec. 7.1: Support Vector Machines </li>
<li> David Sontag's (NYU) slides on SVMs and kernels: lectures 
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture3.pdf> 3</a>, 
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture4.pdf> 4</a>,
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture5.pdf> 5</a> and 
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture6.pdf> 6 </a> 
</ul>
</td></tr>



<tr><td>Sep. 25</td><td>Unsupervised Learning and Dimensionality Reduction</td>
<td>
<a href="lectures/lecture-6.pdf">Lecture 6 slides</a> -
Related material:<br>
<ul>
<li>Bishop 12.1, 12.3: PCA and Kernel PCA </li>
<li>Bishop 9.1: <i>k</i>-means</li>
<li><a href="https://www.cs.nyu.edu/~roweis/lle/papers/lleintro.pdf">Locally Linear Embeddings</a> (optional)</li>
</ul>
</td></tr>

	      
	      
<tr><td>Sep. 27</td><td>Non-Parameteric Models and Gaussian Processes</td>
<td>
<a href="lectures/lecture-7.pdf">Lecture 7 slides</a> -
Related material:<br>
<ul>
<li><a href="http://mlg.eng.cam.ac.uk/zoubin/talks/nips09npb.pdf">Overview of Non-parametric Bayesian Models</a></li>
<li><a href="http://mlss2011.comp.nus.edu.sg/uploads/Site/lect1gp.pdf">Tutorial on Gaussian Processes (why I don't use SVMs)</a></li>
<li><a href="http://videolectures.net/mlss09uk_rasmussen_gp/">Carl Rasmussen's Lecture on Gaussian Processes</a></li>	
<li><a href="http://www.gaussianprocess.org/gpml/">Book on Gaussian Processes</a></li>	
</ul>
</td></tr>

	      
	      
	      
<tr><td>Oct. 2</td><td>Computational Learning Theory</td><td></td></tr>
<tr><td>Oct. 4</td><td>Probabilistic Graphical Models (PGMs)</td><td></td></tr>
<tr><td>Oct. 11</td><td>Inference in PGMs</td><td></td></tr>
<tr><td>Oct. 16</td><td>Latent Variables Models, Gaussian Mixture Models, Expectation Maximization</td><td></td></tr>
<tr><td>Oct. 18</td><td>Time series / Inference in HMMS</td><td></td></tr>
<tr><td>Oct. 23</td><td>Spectral Learning</td><td></td></tr>
<tr><td>Oct. 25</td><td>Learning Dynamical Systems: Bayesain Updating, Kalman Filters...</td><td></td></tr>
<tr><td>Oct. 30</td><td>Approximate Inference in PGMs</td><td></td></tr>
<tr><td>Nov. 1</td><td>Variational Inference</td><td></td></tr>
<tr><td>Nov. 6</td><td>Semi-supervised learning, Active learning.</td><td></td></tr>
<tr><td>Nov. 8</td><td>Neural Networks and co. (RNNs, CNNs, ...)</td><td></td></tr>
<tr><td>Nov. 13</td><td>Generative Adversarial Networks</td><td></td></tr>
<tr><td>Nov. 15</td><td>Tensor Methods in ML</td><td></td></tr>
<tr><td>Nov. 20</td><td>midterm recap</td><td></td></tr>
<tr><td>Nov. 22</td><td>In-class midtern exam</td><td></td></tr>
<tr><td>Nov. 27</td><td>Reinforcement Learning</td><td></td></tr>
<tr><td>Nov. 29</td><td>More on Reinforcement Learning</td><td></td></tr>
<tr><td>Dec. 4</td><td>TBA</td><td></td></tr>
<tr><td>Dec. 6</td><td>TBA</td><td></td></tr>
<tr><td>Dec. 11</td><td>Projects Presentations 1</td><td></td></tr>
<tr><td>Dec. 13</td><td>Projects Presentations 2</td><td></td></tr>


</table>
<p>
&nbsp;
</p>

  </body>
</html>
